# Azure Container Apps
![alt text](https://raw.githubusercontent.com/danielecolon/Azure-ContainerApps-Ollama/refs/heads/main/Ollama.gif)

## Details
Tired of API limits and rising inference costs? Take control of your AI infrastructure! Explore how to run open-source Large Language Models like LLaMA 3, Mistral, and more using Ollama—a tool to run LLMs locally—and Azure Container Apps for scalable, secure deployments.

* Build and deploy Ollama in a containerized environment
* Integrate local LLM inference into your own apps
* Secure, scale (Scale down to zero when not in use!), and monitor AI workloads the cloud-native way
* Whether you’re creating private copilots, intelligent assistants, or exploring AI self-hosting, learn how to own your AI stack—from model to deployment.

## Audience
Developers, cloud engineers, architects, and AI enthusiasts ready to experiment with open-source LLMs in the cloud.

## Speaker
Daniel Colón
Daniel Colón has expertise in cloud, infrastructure, and systems integration obtained through 20 plus years of working as a manager, architect and developer on mission critical back-end systems that have included various cloud platforms such as AWS, Azure, and GCP. He has taught various courses at Northeastern University including ASP.NET, C#, and XML Web Services. He has certifications including A+, Security+, Azure Solutions Architect Expert, AWS Cloud Solutions Architect Professional and GCP Professional Cloud Architect.

### Tags
Cloud Computing<br>
Microsoft Azure<br>
Artificial Intellignece<br>
Containers
